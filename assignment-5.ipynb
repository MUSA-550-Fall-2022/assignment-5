{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions for Assignment 5: Web Scraping and Big Data\n",
    "\n",
    "**NOTE: THIS ASSIGNMENT IS OPTIONAL**\n",
    "\n",
    "You must complete one out of homeworks #4, #5, and #6.\n",
    "\n",
    "**Due date: Monday, 11/14 by the end of the day**\n",
    "\n",
    "The notebook should be submitted to your own private repository on GitHub, which can be created using the following link:\n",
    "\n",
    "\n",
    "\n",
    "### Part 1: Scraping Craigslist\n",
    "\n",
    "In this section, you will use Selenium and BeautifulSoup to scrape data for hundreds of apartments from Philadelphia's Craigslist portal.\n",
    "\n",
    "### Part 2: Exploring a \"large\" dataset of your choice\n",
    "\n",
    "In this section, you'll use datashader to create an animated timelapse of a \"large\" dataset of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Scraping Craigslist\n",
    "\n",
    "In this part, we'll be extracting information on apartments from Craigslist search results. You'll be using Selenium and BeautifulSoup to extract the relevant information from the HTML text. \n",
    "\n",
    "For reference on CSS selectors, please see the [notes from Week 6](https://github.com/MUSA-550-Fall-2022/week-6/blob/main/css-selectors.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primer: the Craigslist website URL\n",
    "\n",
    "We'll start with the Philadelphia region. First we need to figure out how to submit a query to Craigslist. As with many websites, one way you can do this is simply by constructing the proper URL and sending it to Craigslist. \n",
    "\n",
    "https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1~gallery~0~0\n",
    "\n",
    "There are **three** components to this URL. \n",
    "\n",
    "1. The base URL: `http://philadelphia.craigslist.org/search/apa`\n",
    "\n",
    "2. The user's search parameters: `?min_price=1&min_bedrooms=1&minSqft=1`\n",
    "\n",
    "> We will send nonzero defaults for some parameters (bedrooms, size, price) in order to exclude results that have empty values for these parameters.\n",
    "\n",
    "\n",
    "3. The URL *hash*: `#search=1~gallery~0~0`\n",
    "\n",
    "> As we will see later, this part will be important because it contains the search page result number.\n",
    "\n",
    "\n",
    "The Craigslist website requires Javascript, so we'll need to use Selenium to load the page, and then use BeautifulSoup to extract the information we want. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Initialize a selenium driver and open Craigslist\n",
    "\n",
    "As discussed in lecture, you can use Chrome, Firefox, or Edge as your selenium driver. In this part, you should do two things:\n",
    "\n",
    "1. Initialize the selenium driver\n",
    "1. Use the `driver.get()` function to open the following URL:\n",
    "\n",
    "https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1~gallery~0~0\n",
    "\n",
    "This will give you the search results for 1-bedroom apartments in Philadelphia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Initialize your \"soup\"\n",
    "\n",
    "Once selenium has the page open, we can get the page source from the driver and use BeautifulSoup to parse it. In this part, initialize a BeautifulSoup object with the driver's page source\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Parsing the HTML\n",
    "\n",
    "Now that we have our \"soup\" object, we can use BeautifulSoup to extract out the elements we need:\n",
    "\n",
    "- Use the Web Inspector to identify the HTML element that holds the information on each apartment listing.\n",
    "- Use BeautifulSoup to extract these elements from the HTML. \n",
    "\n",
    "\n",
    "At the end of this part, you should have a list of 120 elements, where each element is the listing for a specific apartment on the search page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Find the relevant pieces of information\n",
    "\n",
    "We will now focus on the **first element** in the list of 120 apartments. Use the `prettify()` function to print out the HTML for this first element. \n",
    "\n",
    "From this HTML, identify the HTML elements that hold:\n",
    "\n",
    "- The apartment price\n",
    "- The number of bedrooms\n",
    "- The square footage\n",
    "- The apartment title\n",
    "- The datetime string of the posting, e.g., '2019-03-23 12:07'\n",
    "\n",
    "For the first apartment, print out each of these pieces of information, using BeautifulSoup to select the proper elements.\n",
    "\n",
    "**Hints**\n",
    "- Each of these can be extracted using the `text` attribute of the selected element object, except for the datetime string. This information is stored as an *attribute* of an HTML element and is not part of the displayed text on the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Functions to format the results \n",
    "\n",
    "In this section, you'll create functions that take in the raw string elements for price, size, time, and number of bedrooms and returns them formatted as numbers.\n",
    "\n",
    "I've started the functions to format the values. You should finish theses functions in this section.\n",
    "\n",
    "**Hints**\n",
    "- You can use string formatting functions like `string.replace()` and `string.strip()`\n",
    "- The `int()` and `float()` functions can convert strings to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_bedrooms(bedrooms_string):\n",
    "    # Format the bedrooms string and return an int\n",
    "    # \n",
    "    # This will involve using the string.replace() function to \n",
    "    # remove unwanted characters\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_size(size_string):\n",
    "    # Format the size string and return a float\n",
    "    # \n",
    "    # This will involve using the string.replace() function to \n",
    "    # remove unwanted characters\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_price(price_string):\n",
    "    # Format the price string and return a float\n",
    "    # \n",
    "    # This will involve using the string.strip() function to \n",
    "    # remove unwanted characters\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(date_string):\n",
    "    # Return a Datetime object from the datetime string\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Putting it all together\n",
    "\n",
    "In this part, you'll complete the code block below using results from previous parts. The code will loop over 5 pages of search results and scrape data for 600 apartments. \n",
    "\n",
    "We can get a specific page by changing the `search=PAGE` part of the URL hash. For example, to get page 2 instead of page 1, we will navigate to:\n",
    "\n",
    "\n",
    "https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=2~gallery~0~0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, the outer for loop will loop over 5 pages of search results. The inner for loop will loop over the 120 apartments listed on each search page.\n",
    "\n",
    "Fill in the missing pieces of the inner loop using the code from the previous section. We will be able to extract out the relevant pieces of info for each apartment. \n",
    "\n",
    "After filling in the missing pieces and executing the code cell, you should have a Data Frame called `results` that holds the data for 600 apartment listings.\n",
    "\n",
    "#### Notes\n",
    "\n",
    "Be careful if you try to scrape more listings. Craigslist will temporarily ban your IP address (for a very short time) if you scrape too much at once. I've added a `sleep()` function to the for loop to wait 30 seconds between scraping requests.\n",
    "\n",
    "If the for loop gets stuck at the \"Processing page X...\" step for more than a minute or so, your IP address is probably banned temporarily, and you'll have to wait a few minutes before trying again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 1...\n",
      "Number of apartments =  120\n",
      "sleeping for 10 seconds between calls\n",
      "Processing page 2...\n",
      "Number of apartments =  120\n",
      "sleeping for 10 seconds between calls\n",
      "Processing page 3...\n",
      "Number of apartments =  120\n",
      "sleeping for 10 seconds between calls\n",
      "Processing page 4...\n",
      "Number of apartments =  120\n",
      "sleeping for 10 seconds between calls\n",
      "Processing page 5...\n",
      "Number of apartments =  120\n",
      "sleeping for 10 seconds between calls\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# search in batches of 120 for 5 pages\n",
    "# NOTE: you will get temporarily banned if running more than ~5 pages or so\n",
    "# the API limits are more leninient during off-peak times, and you can try\n",
    "# experimenting with more pages\n",
    "max_pages = 5\n",
    "\n",
    "# The base URL we will be using\n",
    "base_url = \"https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1\"\n",
    "\n",
    "# loop over each page of search results\n",
    "for page_num in range(1, max_pages + 1):\n",
    "    print(f\"Processing page {page_num}...\")\n",
    "\n",
    "    # Update the URL hash for this page number and make the combined URL\n",
    "    url_hash = f\"#search={page_num}~gallery~0~0\"\n",
    "    url = base_url + url_hash\n",
    "\n",
    "    # Go to the driver and wait for 5 seconds\n",
    "    driver.get(url)\n",
    "    sleep(5)\n",
    "\n",
    "    # YOUR CODE: get the list of all apartments\n",
    "    # This is the same code from Part 1.2 and 1.3\n",
    "    # It should be a list of 120 apartments\n",
    "    soup = \n",
    "    apts = \n",
    "    print(\"Number of apartments = \", len(apts))\n",
    "\n",
    "    # loop over each apartment in the list\n",
    "    page_results = []\n",
    "    for apt in apts:\n",
    "\n",
    "        # YOUR CODE: the bedrooms string\n",
    "        bedrooms = \n",
    "\n",
    "        # YOUR CODE: the size string\n",
    "        size = \n",
    "\n",
    "        # YOUR CODE: the title string\n",
    "        title = \n",
    "\n",
    "        # YOUR CODE: the price string\n",
    "        price = \n",
    "\n",
    "        # YOUR CODE: the time string\n",
    "        dtime = \n",
    "\n",
    "        # Format using functions from Part 1.5\n",
    "        bedrooms = format_bedrooms(bedrooms)\n",
    "        size = format_size(size)\n",
    "        price = format_price(price)\n",
    "        dtime = format_time(dtime)\n",
    "\n",
    "        # Save the result\n",
    "        page_results.append([dtime, price, size, bedrooms, title])\n",
    "\n",
    "    # Create a dataframe and save\n",
    "    col_names = [\"time\", \"price\", \"size\", \"bedrooms\", \"title\"]\n",
    "    df = pd.DataFrame(page_results, columns=col_names)\n",
    "    results.append(df)\n",
    "\n",
    "    print(\"sleeping for 10 seconds between calls\")\n",
    "    sleep(10)\n",
    "\n",
    "# Finally, concatenate all the results\n",
    "results = pd.concat(results, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Plotting the distribution of prices\n",
    "\n",
    "Use matplotlib's `hist()` function to make two histograms for:\n",
    "\n",
    "- Apartment prices\n",
    "- Apartment prices per square foot (price / size)\n",
    "\n",
    "Make sure to add labels to the respective axes and a title describing the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side note: rental prices per sq. ft. from Craigslist\n",
    "\n",
    "The histogram of price per sq ft should be centered around ~1.5. Here is a plot of how Philadelphia's rents compare to the other most populous cities:\n",
    "\n",
    "<img src=\"imgs/rental_prices_psf.png\" width=600/>\n",
    "\n",
    "[Source](https://arxiv.org/pdf/1605.05397.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Comparing prices for different sizes \n",
    "\n",
    "Use `altair` to explore the relationship between price, size, and number of bedrooms. Make an interactive scatter plot of price (x-axis) vs. size (y-axis), with the points colored by the number of bedrooms. \n",
    "\n",
    "Make sure the plot is interactive (zoom-able and pan-able) and add a tooltip with all of the columns in our scraped data frame.\n",
    "\n",
    "With this sort of plot, you can quickly see the outlier apartments in terms of size and price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Making an animated map with Datashader\n",
    "\n",
    "In this part, you will load a \"large\" data set of your choice and make an animated map of the data using datashader. \n",
    "\n",
    "There are several good options available from Open Data Philly, but you are welcome to choose a different source, as long it meets the requirements below.\n",
    "\n",
    "#### Deliverable\n",
    "Your final result should be a GIF animating changes in the data set in time. This GIF should be submitted to your repository, and the code used to produce the GIF should be in the notebook.\n",
    "\n",
    "#### Notes\n",
    "- For best results with datashader, the dataset should be at least 500,000 rows in size. If you are having difficulty finding a data set large enough, please email me.\n",
    "- The data must have a date time column, so that you can animate the data in time.\n",
    "- You can use either dask or pandas to load the data. If you're data set is approaching the size of your machine's memory, you will want to use dask.\n",
    "- Recommendations from Open Data Philly include: \n",
    "    - [311 Requests](https://www.opendataphilly.org/dataset/311-service-and-information-requests)\n",
    "    - [Crime Incidents](https://www.opendataphilly.org/dataset/crime-incidents)\n",
    "    - [L&I Code Violations](https://www.opendataphilly.org/dataset/licenses-and-inspections-violations)\n",
    "    - [Real Estate Transfers](https://www.opendataphilly.org/dataset/real-estate-transfers)\n",
    "- You can animate the data by the hour or by the year, e.g., parking violations by hour of day, or parking violations by year.\n",
    "\n",
    "#### Examples\n",
    "I've includes several potential examples in the assignment repository.\n",
    "\n",
    "#### Optional\n",
    "You can overlay Philadelphia city limits using geopandas. See the lecture slides for an example of how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
